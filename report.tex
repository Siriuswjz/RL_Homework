\documentclass[12pt,a4paper]{article}
\usepackage{ctex}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{fontspec}

\setmainfont{Times New Roman}

\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}
\setlength{\parindent}{2em}
\linespread{1.0}

\lstset{
    basicstyle=\small\ttfamily,
    breaklines=true,
    frame=single,
    numbers=left,
    numberstyle=\tiny,
}

\title{\textbf{基于DQN的无人机自主导航技术报告}}
\author{ZY2557445 汪谨遵}
\date{}

\begin{document}

\maketitle

\section{引言}

本项目实现了基于深度Q网络(Deep Q-Network, DQN)的无人机自主导航系统。无人机需要在AirSim仿真环境中学习穿越一系列圆形洞口，避免与障碍物碰撞。该任务具有挑战性，要求智能体能够从RGB图像中提取空间特征，并做出准确的飞行决策。

\section{算法描述}

\subsection{DQN算法原理}

DQN是一种结合深度学习与强化学习的算法，通过神经网络近似Q函数来学习最优策略。核心思想是最小化时序差分误差：

\begin{equation}
L(\theta) = \mathbb{E}_{(s,a,r,s')\sim D}\left[\left(r + \gamma \max_{a'}Q(s',a';\theta^-) - Q(s,a;\theta)\right)^2\right]
\end{equation}

其中$\theta$为当前网络参数，$\theta^-$为目标网络参数，$D$为经验回放缓冲区，$\gamma$为折扣因子。

\subsection{网络架构}

本项目采用VGG16作为特征提取器，并引入注意力机制增强空间感知能力。网络结构如下：

\begin{itemize}
    \item \textbf{特征提取层：}使用VGG16前23层卷积层，保留更多空间信息
    \item \textbf{注意力模块：}通过$1\times1$卷积生成注意力权重，突出关键区域
    \item \textbf{全连接层：}三层全连接网络[512, 512, 256]，输出9个动作的Q值
\end{itemize}

特征提取器采用预训练权重，前10层参数冻结，后续层进行微调，平衡迁移学习与任务适应。

\subsection{动作空间}

智能体可执行9个离散动作，对应无人机在YZ平面的移动方向：

\begin{table}[h]
\centering
\begin{tabular}{ccc}
\toprule
动作编号 & Y方向速度 & Z方向速度 \\
\midrule
0 & -0.4 & -0.4 \\
1 & 0 & -0.4 \\
2 & 0.4 & -0.4 \\
3 & -0.4 & 0 \\
4 & 0 & 0 \\
5 & 0.4 & 0 \\
6 & -0.4 & 0.4 \\
7 & 0 & 0.4 \\
8 & 0.4 & 0.4 \\
\bottomrule
\end{tabular}
\caption{动作空间定义}
\end{table}

无人机X方向保持恒定速度0.4 m/s前进。

\subsection{奖励函数}

奖励函数设计综合考虑目标接近度、对齐精度和任务完成情况：

\begin{equation}
R = \begin{cases}
20 \times (\Delta d_{prev} - \Delta d_{curr}) + R_{align} & \text{正常飞行} \\
-100 & \text{碰撞} \\
10 & \text{成功穿越} \\
-100 & \text{目标丢失}
\end{cases}
\end{equation}

其中$\Delta d$为无人机与目标中心的距离，$R_{align}$为对齐奖励：

\begin{equation}
R_{align} = \begin{cases}
19 & d < 0.30 \text{ 且 } x > 2.9 \\
12 & d < 0.30 \\
7 & d < 0.45 \\
0 & \text{其他}
\end{cases}
\end{equation}

\section{实验设置}

\subsection{环境配置}

\begin{itemize}
    \item \textbf{仿真平台：}Microsoft AirSim + Unreal Engine 4
    \item \textbf{观测空间：}$50\times50\times3$ RGB图像
    \item \textbf{仿真加速：}20倍速（ClockSpeed=20）
    \item \textbf{洞口间距：}4米
    \item \textbf{洞口半径：}0.3米
\end{itemize}

\subsection{训练参数}

\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
参数 & 数值 \\
\midrule
总训练步数 & 500,000 \\
学习率 & $3\times10^{-4}$ \\
批次大小 & 64 \\
经验回放缓冲区 & 100,000 \\
折扣因子$\gamma$ & 0.99 \\
探索率衰减 & 线性衰减(1.0$\rightarrow$0.02) \\
目标网络更新频率 & 10,000步 \\
评估频率 & 1,000步 (0-390K) / 10,000步 (390K-500K) \\
优化器 & Adam \\
\bottomrule
\end{tabular}
\caption{DQN训练超参数}
\end{table}

\textbf{评估频率调整说明：}训练初期采用较高的评估频率（每1,000步），以密切监控模型性能。但频繁评估显著降低了训练速度，前390K步耗时超过1天。因此在390K步后将评估频率降低至10,000步，在保证性能监控的同时大幅提升训练效率，最终110K步仅需约4小时完成。

\subsection{硬件环境}

\begin{itemize}
    \item \textbf{GPU：}NVIDIA RTX 5060
    \item \textbf{框架：}PyTorch 1.7.1, Stable-Baselines3 1.2.0
    \item \textbf{总训练时间：}约3天（含评估频率优化前后和第二阶段训练）
\end{itemize}

\section{实验结果}

\subsection{单洞训练（第一阶段）}

第一阶段采用标准DQN训练，每次穿越一个洞后重置环境。训练过程中，模型性能稳步提升。图\ref{fig:training_curves}展示了训练过程中的关键指标变化。

\begin{figure}[htbp]
\centering
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/eval_reward.png}
\caption{评估奖励变化}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/eval_length.png}
\caption{Episode长度变化}
\end{subfigure}

\vspace{0.5cm}

\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/train_reward.png}
\caption{训练奖励变化}
\end{subfigure}
\caption{单洞训练过程关键指标变化曲线}
\label{fig:training_curves}
\end{figure}

从图\ref{fig:training_curves}可以看出：

\begin{itemize}
    \item \textbf{评估奖励：}从初始的负值逐步提升至150之间，并在后期保持稳定
    \item \textbf{Episode长度：}从初始的0步左右提升至10-11步，表明智能体学会了更精确的导航
    \item \textbf{训练奖励：}整体呈上升趋势，验证了学习过程的有效性
    \item \textbf{收敛性：}约在300K步后模型性能趋于稳定，表明训练充分
\end{itemize}

使用最佳模型进行177个episode的测试，结果如下：

\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
指标 & 数值 \\
\midrule
总测试episodes & 177 \\
平均飞行距离 & 25.67 m \\
最大飞行距离 & 102.74 m \\
平均穿洞数 & 6 \\
最大穿洞数 & 25 \\
单洞成功率 & 60-70\% \\
\bottomrule
\end{tabular}
\caption{单洞训练模型测试结果}
\end{table}

\subsection{多洞训练（第二阶段）}

为进一步提升连续穿洞能力，进行了第二阶段训练。该阶段采用迁移学习策略，加载第一阶段最佳模型权重，在多洞连续穿越环境中继续训练。

\subsubsection{训练配置}

\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
参数 & 数值 \\
\midrule
总训练步数 & 600,000 \\
学习率 & $1\times10^{-4}$ \\
初始探索率 & 0.2 \\
预训练模型 & 第一阶段best\_model \\
环境类型 & MultiHoleEnv \\
奖励函数 & 连续穿洞累积奖励 \\
\bottomrule
\end{tabular}
\caption{多洞训练超参数}
\end{table}

多洞环境的奖励函数设计为：

\begin{equation}
R_{multi} = R_{base} + \begin{cases}
50 + n \times 10 & \text{穿越第}n\text{个洞} \\
-100 & \text{碰撞}
\end{cases}
\end{equation}

其中$R_{base}$为基础导航奖励，$n$为已穿越洞口数量。

\subsubsection{训练过程}

图\ref{fig:multi_hole_training}展示了多洞训练的评估奖励变化曲线。

\begin{figure}[htbp]
\centering
\includegraphics[width=0.7\textwidth]{figures/multi_hole_eval_reward.png}
\caption{多洞训练评估奖励变化曲线}
\label{fig:multi_hole_training}
\end{figure}

\subsubsection{测试结果}

使用多洞训练的最佳模型进行709个episode的测试：

\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
指标 & 数值 \\
\midrule
总测试episodes & 709 \\
平均飞行距离 & 18.40 m \\
最大飞行距离 & 43.16 m \\
平均穿洞数 & 4 \\
最大穿洞数 & 10 \\
\bottomrule
\end{tabular}
\caption{多洞训练模型测试结果}
\end{table}

\subsection{两阶段对比分析}

\subsubsection{性能对比}

图\ref{fig:comparison}展示了单洞训练与多洞训练的评估奖励对比。

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{figures/comparison.png}
\caption{单洞训练 vs 多洞训练评估奖励对比}
\label{fig:comparison}
\end{figure}

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
指标 & 单洞训练 & 多洞训练 \\
\midrule
平均飞行距离 & 25.67 m & 18.40 m \\
最大飞行距离 & 102.74 m & 43.16 m \\
平均穿洞数 & 6 & 4 \\
最大穿洞数 & 25 & 10 \\
\bottomrule
\end{tabular}
\caption{两阶段训练结果对比}
\end{table}

对比结果显示，多洞训练模型的性能反而下降。平均穿洞数从6降至4，最大穿洞数从25降至10。这一反直觉的结果值得深入分析。

\subsection{性能下降原因分析}

通过对比图\ref{fig:comparison}中的训练曲线和测试结果，可以从以下几个方面分析多洞训练性能下降的原因：

\subsubsection{1. 过拟合问题}

从图\ref{fig:comparison}可以看出，多洞训练的评估奖励在训练环境中持续上升，但测试性能却显著下降。这是典型的过拟合现象。多洞训练环境采用固定的起始位置和洞口分布，导致模型过度适应训练环境的特定模式，而失去了对新环境的泛化能力。

单洞训练通过每次穿洞后重置环境，引入了更多的随机性和多样性，使模型学到的是更通用的导航策略。而多洞训练的连续穿洞机制虽然增加了任务复杂度，但也限制了环境多样性。

\subsubsection{2. 奖励函数设计缺陷}

多洞环境的累积奖励机制存在以下问题：

\begin{itemize}
    \item \textbf{奖励稀疏：}模型需要连续穿越多个洞才能获得显著奖励，增加了学习难度
    \item \textbf{奖励延迟：}穿洞奖励的累积特性导致信用分配问题，模型难以判断哪些动作真正有效
    \item \textbf{保守策略：}高额的碰撞惩罚(-100)可能引导模型采取过于保守的策略，优先避免碰撞而非追求更多穿洞
\end{itemize}

相比之下，单洞训练的奖励函数更加平衡，既鼓励接近目标，又给予穿洞即时奖励，使学习过程更加高效。

\subsubsection{3. 探索-利用失衡}

第二阶段将初始探索率降至0.2，虽然有利于利用已学知识，但在更复杂的多洞环境中，这一设置可能限制了对新策略的探索。从图\ref{fig:comparison}可以看出，多洞训练曲线的波动较大，说明模型在探索和利用之间未能找到良好平衡。

单洞训练从1.0线性衰减至0.02的探索策略，为模型提供了充分的探索空间，使其能够发现更优的导航策略。

\subsubsection{4. 灾难性遗忘}

迁移学习过程中，模型在适应新任务时可能遗忘了第一阶段学到的基础导航技能。虽然采用了较低的学习率($1\times10^{-4}$)来保护已学知识，但在600K步的长时间训练中，仍无法完全避免遗忘现象。

这一问题在图\ref{fig:comparison}中表现为多洞训练曲线在后期出现波动，说明模型在新旧知识之间产生了冲突。

\subsubsection{5. 训练-测试环境不匹配}

多洞训练环境的动态特性（连续穿洞、位置更新）与测试环境存在本质差异。测试环境采用碰撞后随机重置的机制，而多洞训练环境则是连续前进。这种不匹配导致模型在训练中学到的"连续穿洞"策略无法有效迁移到测试环境。

单洞训练环境与测试环境的机制更加一致，都是单次穿洞后重置，因此模型的泛化能力更强。

\subsection{模型选择}

综合考虑性能和泛化能力，选择第一阶段的\texttt{best\_model.zip}作为最终部署模型。该模型在单洞训练中学到的基础导航技能更加稳定，泛化能力更强，在实际测试中表现出更优的性能。

\section{技术亮点}

\subsection{特征提取优化}

\begin{enumerate}
    \item \textbf{迁移学习：}使用ImageNet预训练的VGG16权重，加速收敛
    \item \textbf{注意力机制：}引入空间注意力模块，提升对洞口位置的感知能力
    \item \textbf{分层微调：}冻结浅层特征，仅微调深层特征，防止过拟合
\end{enumerate}

\subsection{训练策略}

\begin{enumerate}
    \item \textbf{经验回放：}打破样本相关性，提高样本利用效率
    \item \textbf{目标网络：}稳定训练过程，避免Q值估计震荡
    \item \textbf{评估驱动：}定期评估并保存最佳模型，确保泛化性能
    \item \textbf{断点续训：}支持训练中断后自动恢复，提高训练灵活性
\end{enumerate}

\section{结论}

本项目成功实现了基于DQN的无人机自主导航系统。通过结合VGG16特征提取器和注意力机制，模型能够从RGB图像中学习有效的导航策略。

实验进行了两个阶段的训练：
\begin{enumerate}
    \item \textbf{单洞训练：}模型学习基础穿洞技能，平均穿洞数6个，最大25个
    \item \textbf{多洞训练：}尝试通过迁移学习提升连续穿洞能力，但出现性能下降
\end{enumerate}

多洞训练的性能下降揭示了强化学习中的几个关键问题：过拟合、奖励函数设计、探索-利用平衡、灾难性遗忘等。这些问题在迁移学习和任务泛化中普遍存在，需要更精细的算法设计和训练策略。

最终选择单洞训练的\texttt{best\_model.zip}作为部署模型，该模型展现出更强的泛化能力和稳定性。

\end{document}
